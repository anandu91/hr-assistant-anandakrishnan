# ğŸ¤– AI-Powered HR Assistant (Streamlit + Mistral via Ollama)

This project is an **AI-powered HR assistant** that lets you ask natural language questions about your employee dataset. It combines:
- ğŸ“Š **Pandas & Rule-based Parsing**
- ğŸ’¡ **Local LLMs** using [Mistral 7B via Ollama](https://ollama.com/)
- âš™ï¸ **Streamlit** for a fast, interactive UI

You can upload your own CSV file and get instant answers â€” from headcount, join/exit filters, salary stats, to department-level breakdowns!

---

## ğŸ“º Demo Video

ğŸ”— **Watch on YouTube**: [https://youtu.be/ZDJxEy8Dvwk](https://youtu.be/ZDJxEy8Dvwk)

[![Watch the demo](Sample_demo/app_interface.png)](https://youtu.be/ZDJxEy8Dvwk)

---

## ğŸ“¸ Screenshots

### âœ… Application Interface  
![App Interface](Sample_demo/app_interface.png)

---

## ğŸ›  Features

- ğŸ” Upload any employee dataset (`.csv`)
- ğŸ’¬ Ask natural language questions like:
  - â€œHow many employees joined in 2024?â€
  - â€œShow me average salary by departmentâ€
  - â€œWho exited in the last 6 months?â€
- ğŸ“Š Get visual charts for trends like salary distribution, department counts, etc.
- âš¡ Works fully **offline** using **local LLMs** via [Ollama](https://ollama.com)

---

## ğŸ“ Project Structure

â”œâ”€â”€ app.py â† Main Streamlit app
â”œâ”€â”€ data/
â”‚ â””â”€â”€ employee_data.csv â† Sample HR dataset
â”œâ”€â”€ helpers/
â”‚ â”œâ”€â”€ llm_handler.py â† LLM interface (Mistral via Ollama)
â”‚ â””â”€â”€ query_handler.py â† Rule-based query parser
â”œâ”€â”€ Sample_demo/
â”‚ â”œâ”€â”€ app_interface.png â† Screenshot of app
â”‚ â””â”€â”€ Sample_demo.mp4 â† Optional video demo (YouTube used instead)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## ğŸš€ How to Run the Project Locally

### 1. Clone the Repo

```bash
git clone https://github.com/anandu91/hr-assistant-anandakrishnan.git
cd hr-assistant-anandakrishnan


2. Create and Activate Virtual Environment

python -m venv venv
.\venv\Scripts\activate    # For Windows
# Or:
# source venv/bin/activate  # For Mac/Linux

3. Install Dependencies

pip install -r requirements.txt

4. Start the Ollama Model

Make sure you have Ollama installed and pull the model:

ollama run mistral

ğŸ’¡ You can replace mistral with any compatible local LLM model.

5. Run the Streamlit App

streamlit run app.py

You will see a web interface open at http://localhost:8501 where you can upload a CSV and ask queries.

ğŸ’¼ Sample Dataset

Provided in data/employee_data.csv


